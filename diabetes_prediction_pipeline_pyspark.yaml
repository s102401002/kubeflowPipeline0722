# PIPELINE DEFINITION
# Name: diabetes-prediction-pipeline-with-pyspark
# Description: Using PySpark to train and evaluate a diabetes prediction model
# Inputs:
#    nfs_mount_path: str [Default: '/mnt/datasets']
# Outputs:
#    Output: system.Artifact
components:
  comp-evaluate-model:
    executorLabel: exec-evaluate-model
    inputDefinitions:
      artifacts:
        model_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        x_test:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        y_test:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        result_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-load-data:
    executorLabel: exec-load-data
    inputDefinitions:
      parameters:
        nas_mount_path:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        data_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-prepare-data:
    executorLabel: exec-prepare-data
    inputDefinitions:
      artifacts:
        data_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        x_test_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        x_train_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        y_test_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        y_train_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      artifacts:
        x_train:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        y_train:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        train_model_output:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-evaluate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pyspark==3.3.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(model_path: Input[Model], x_test: Input[Artifact],\
          \ y_test: Input[Artifact], result_output: Output[Artifact]):\n    from pyspark.sql\
          \ import SparkSession\n    from pyspark.ml.classification import RandomForestClassificationModel\n\
          \    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\
          \    import joblib\n    import pandas as pd\n\n    spark = SparkSession.builder.appName(\"\
          DiabetesPrediction\").getOrCreate()\n\n    # Load model\n    model = joblib.load(filename=model_path.path)\n\
          \n    # Load test data\n    x_test_df = spark.read.csv(x_test.path, header=True,\
          \ inferSchema=True)\n    y_test_df = spark.read.csv(y_test.path, header=True,\
          \ inferSchema=True)\n\n    # Combine features and labels\n    test_data\
          \ = x_test_df.withColumn('label', y_test_df['diabetes'])\n\n    # Predict\
          \ and evaluate\n    predictions = model.transform(test_data)\n    evaluator\
          \ = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\
          \ metricName='accuracy')\n    accuracy = evaluator.evaluate(predictions)\n\
          \n    # Write the result to a file\n    result_df = pd.DataFrame({'accuracy':\
          \ [accuracy]})\n    result_df.to_csv(result_output.path, index=False)\n\n"
        image: python:3.9
    exec-load-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pyspark==3.3.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_data(nas_mount_path: str, data_output: Output[Artifact]):\n\
          \    from pyspark.sql import SparkSession\n\n    spark = SparkSession.builder.appName(\"\
          DiabetesPrediction\").getOrCreate()\n\n    # Read CSV files from the directory\n\
          \    df = spark.read.csv(nas_mount_path + '/*.csv', header=True, inferSchema=True)\n\
          \n    # Define standard name mapping\n    standard_name_mapping = {\n  \
          \      'gender': ['gender', 'gen', 'Gender', 'sex', 'Sex'],\n        'age':\
          \ ['age', 'Age', 'AGE'],\n        'bmi': ['bmi', 'BMI', 'Bmi'],\n      \
          \  'HbA1c_level': ['HbA1c_level', 'HbA1c', 'hba1c'],\n        'blood_glucose_level':\
          \ ['blood_glucose_level', 'glucose', 'BloodGlucose'],\n        'diabetes':\
          \ ['diabetes', 'Diabetes']\n    }\n\n    # Rename columns based on the standard\
          \ name mapping\n    for standard_name, variants in standard_name_mapping.items():\n\
          \        for variant in variants:\n            if variant in df.columns:\n\
          \                df = df.withColumnRenamed(variant, standard_name)\n   \
          \             break\n\n    # Drop rows where 'diabetes' is 'No Info'\n \
          \   df = df.filter(df['diabetes'] != 'No Info')\n\n    # Drop rows with\
          \ missing values\n    df = df.dropna(thresh=4)\n\n    # Map gender values\
          \ to numerical\n    df = df.withColumn('gender', when(col('gender') == 'Male',\
          \ 0)\n                                  .when(col('gender') == 'Female',\
          \ 1)\n                                  .otherwise(None))\n\n    # Fill\
          \ missing values\n    df = df.na.fill({\n        'age': df.agg(mean('age')).first()[0],\n\
          \        'bmi': df.agg(mean('bmi')).first()[0],\n        'HbA1c_level':\
          \ df.agg(mean('HbA1c_level')).first()[0],\n        'blood_glucose_level':\
          \ df.agg(mean('blood_glucose_level')).first()[0]\n    })\n\n    # Save to\
          \ CSV\n    df.toPandas().to_csv(data_output.path, index=False)\n\n"
        image: python:3.9
    exec-prepare-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - prepare_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pyspark==3.3.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef prepare_data(\n    data_input: Input[Artifact], \n    x_train_output:\
          \ Output[Artifact], x_test_output: Output[Artifact],\n    y_train_output:\
          \ Output[Artifact], y_test_output: Output[Artifact]\n):\n    from pyspark.sql\
          \ import SparkSession\n    from pyspark.ml.feature import VectorAssembler\n\
          \    from pyspark.ml import Pipeline\n    from pyspark.ml.linalg import\
          \ Vectors\n    from pyspark.ml.classification import RandomForestClassifier\n\
          \    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\
          \n    spark = SparkSession.builder.appName(\"DiabetesPrediction\").getOrCreate()\n\
          \n    # Load data\n    df_data = spark.read.csv(data_input.path, header=True,\
          \ inferSchema=True)\n\n    # Prepare features and labels\n    feature_columns\
          \ = ['gender', 'age', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n   \
          \ assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n\
          \    df_data = assembler.transform(df_data)\n\n    # Split data\n    train_data,\
          \ test_data = df_data.randomSplit([0.8, 0.2], seed=42)\n\n    # Separate\
          \ features and labels\n    train_data = train_data.select('features', 'diabetes')\n\
          \    test_data = test_data.select('features', 'diabetes')\n\n    # Save\
          \ to CSV\n    train_data.toPandas().to_csv(x_train_output.path, index=False)\n\
          \    test_data.toPandas().to_csv(x_test_output.path, index=False)\n    #\
          \ Save labels separately\n    train_data.select('diabetes').toPandas().to_csv(y_train_output.path,\
          \ index=False)\n    test_data.select('diabetes').toPandas().to_csv(y_test_output.path,\
          \ index=False)\n\n"
        image: python:3.9
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pyspark==3.3.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(x_train: Input[Artifact], y_train: Input[Artifact],\
          \ train_model_output: Output[Model]):\n    from pyspark.sql import SparkSession\n\
          \    from pyspark.ml.classification import RandomForestClassifier\n    from\
          \ pyspark.ml import Pipeline\n    import joblib\n\n    spark = SparkSession.builder.appName(\"\
          DiabetesPrediction\").getOrCreate()\n\n    # Load data\n    x_train_df =\
          \ spark.read.csv(x_train.path, header=True, inferSchema=True)\n    y_train_df\
          \ = spark.read.csv(y_train.path, header=True, inferSchema=True)\n\n    #\
          \ Combine features and labels\n    train_data = x_train_df.withColumn('label',\
          \ y_train_df['diabetes'])\n\n    # Train the model\n    rf = RandomForestClassifier(featuresCol='features',\
          \ labelCol='label')\n    model = rf.fit(train_data)\n\n    # Save the model\n\
          \    joblib.dump(model, train_model_output.path)\n\n"
        image: python:3.9
pipelineInfo:
  description: Using PySpark to train and evaluate a diabetes prediction model
  name: diabetes-prediction-pipeline-with-pyspark
root:
  dag:
    outputs:
      artifacts:
        Output:
          artifactSelectors:
          - outputArtifactKey: result_output
            producerSubtask: evaluate-model
    tasks:
      evaluate-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-model
        dependentTasks:
        - prepare-data
        - train-model
        inputs:
          artifacts:
            model_path:
              taskOutputArtifact:
                outputArtifactKey: train_model_output
                producerTask: train-model
            x_test:
              taskOutputArtifact:
                outputArtifactKey: x_test_output
                producerTask: prepare-data
            y_test:
              taskOutputArtifact:
                outputArtifactKey: y_test_output
                producerTask: prepare-data
        taskInfo:
          name: evaluate-model
      load-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-load-data
        inputs:
          parameters:
            nas_mount_path:
              componentInputParameter: nfs_mount_path
        taskInfo:
          name: load-data
      prepare-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-prepare-data
        dependentTasks:
        - load-data
        inputs:
          artifacts:
            data_input:
              taskOutputArtifact:
                outputArtifactKey: data_output
                producerTask: load-data
        taskInfo:
          name: prepare-data
      train-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-model
        dependentTasks:
        - prepare-data
        inputs:
          artifacts:
            x_train:
              taskOutputArtifact:
                outputArtifactKey: x_train_output
                producerTask: prepare-data
            y_train:
              taskOutputArtifact:
                outputArtifactKey: y_train_output
                producerTask: prepare-data
        taskInfo:
          name: train-model
  inputDefinitions:
    parameters:
      nfs_mount_path:
        defaultValue: /mnt/datasets
        isOptional: true
        parameterType: STRING
  outputDefinitions:
    artifacts:
      Output:
        artifactType:
          schemaTitle: system.Artifact
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.8.0
